<h1 id="carnd-capstone-project">CarND-Capstone-Project</h1>
<h2 id="programming-a-real-self-driving-car">Programming a Real Self Driving Car</h2>
<hr />
<ul>
<li><a href="#sec-1">Team: Autowheels</a></li>
<li><a href="#sec-2">Overview</a></li>
<li><a href="#sec-3">System Architecture</a></li>
<li><a href="#sec-4">Installation steps</a></li>
<li><a href="#sec-5">Usage</a></li>
<li><a href="#sec-6">Traffic Light Detection and Classification End-to-End Approach using Tensorflow</a></li>
<li><a href="#sec-6-1">Development Overview</a></li>
<li><a href="#sec-6-2">Performance Evaluation</a></li>
<li><a href="#sec-6-3">PID Tuning Parameters</a></li>
<li><a href="#sec-7">Results</a></li>
<li><a href="#sec-8">License</a></li>
</ul>
<hr />
<h2 id="meet-team-autowheels">Meet Team AutoWheels<a id="sec-1"></a></h2>
<p>Team AutoWheels has five members. Below are their names, email addresses and slack handles.</p>
<table>
<thead>
<tr class="header">
<th align="left">Team Member Name</th>
<th align="left">Email Address</th>
<th align="left">Slack Handle</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Diogo Silva (Team Lead)</td>
<td align="left">akins.daos+selfdriving@gmail.com</td>
<td align="left"><span class="citation">@diogoaos</span></td>
</tr>
<tr class="even">
<td align="left">Volker van Aken</td>
<td align="left">volker.van.aken@gmail.com</td>
<td align="left"><span class="citation">@Volker</span></td>
</tr>
<tr class="odd">
<td align="left">Andreea Patachi</td>
<td align="left">patachiandreea@yahoo.com</td>
<td align="left"><span class="citation">@Andreea</span></td>
</tr>
<tr class="even">
<td align="left">Stephen Nutman</td>
<td align="left">stephen_nutman@outlook.com</td>
<td align="left"><span class="citation">@Steve</span></td>
</tr>
<tr class="odd">
<td align="left">Alexander Meade</td>
<td align="left">alexander.n.meade@gmail.com</td>
<td align="left"><span class="citation">@ameade</span></td>
</tr>
</tbody>
</table>
<h2 id="overview">Overview<a id="sec-2"></a></h2>
<p>This is the final project in the Udacity Self Driving Car NanoDegree course. The task of this Capstone project was to create ROS nodes to implement core functionality of an autonomous vehicle system, including traffic light detection, vehicle control and waypoint path following. The development uses a simulator to support in evaluating the code performance. Once ready to run there was an opportunity to run the code on a real car - the Udacity AD vehicle Carla.</p>
<h2 id="system-architecture-diagram">System Architecture Diagram<a id="sec-3"></a></h2>
<p>The following system diagram shows the architecture of the code that was implemented. The architecture is split into 3 main areas:</p>
<ul>
<li>Perception (Traffic Light Detection)</li>
<li>Planning (Waypoint Following)</li>
<li>Control (Vehicle longitudinal and lateral control)</li>
</ul>
<p>From this diagram the ROS topics can be seen communicating between the ROS nodes. Information is also passed on these topics to the Car simulator.</p>
<div class="figure">
<img src="./support/structure.png" title="Program Structure" alt="alt text" />
<p class="caption">alt text</p>
</div>
<h2 id="installation-steps">Installation steps<a id="sec-4"></a></h2>
<ul>
<li>Be sure that your workstation is running Ubuntu 16.04 Xenial Xerus or Ubuntu 14.04 Trusty Tahir. <a href="https://www.ubuntu.com/download/desktop">Ubuntu downloads can be found here</a>.</li>
<li>If using a Virtual Machine to install Ubuntu, use the following configuration as minimum:</li>
<li>2 CPU</li>
<li>2 GB system memory</li>
<li><p>25 GB of free hard drive space</p></li>
<li>Follow these instructions to install ROS</li>
<li><a href="http://wiki.ros.org/kinetic/Installation/Ubuntu">ROS Kinetic</a> if you have Ubuntu 16.04.</li>
<li><a href="http://wiki.ros.org/indigo/Installation/Ubuntu">ROS Indigo</a> if you have Ubuntu 14.04.</li>
<li><a href="https://bitbucket.org/DataspeedInc/dbw_mkz_ros">Dataspeed DBW</a></li>
<li>Use this option to install the SDK on a workstation that already has ROS installed:<a href="https://bitbucket.org/DataspeedInc/dbw_mkz_ros/src/81e63fcc335d7b64139d7482017d6a97b405e250/ROS_SETUP.md?fileviewer=file-view-default">One Line SDK Install (binary)</a></li>
<li><p>Download the <a href="https://github.com/udacity/CarND-Capstone/releases">Udacity Simulator</a>.</p></li>
</ul>
<h2 id="usage">Usage<a id="sec-5"></a></h2>
<ol style="list-style-type: decimal">
<li>Make a project directory <code>mkdir project_udacity &amp;&amp; cd project_udacity</code></li>
<li>Clone this repository into the project_udacity directory. <code>https://github.com/nutmas/CarND-Capstone.git</code></li>
<li>Install python dependencies. <code>cd CarND-Capstone-Project\</code> and <code>pip install -r requirements.txt</code> will install dependencies.</li>
<li>Build code. <code>cd ros\</code> and <code>catkin_make</code> and <code>source devel/setup.sh</code></li>
<li>Create a directory for simulator <code>cd</code> and 'mkdir Sim<code>and</code>cd Sim`</li>
<li>Download Simulator from here: <a href="https://github.com/udacity/CarND-Capstone/releases">Udacity Simulator</a></li>
<li>Run the simulator <code>cd linux_sys_int</code> and <code>./sys_int.x86_64</code> (for linux 64bit system)</li>
</ol>
<div class="figure">
<img src="./support/SimulatorStartUp.png" title="Simulator Welcome" alt="alt text" />
<p class="caption">alt text</p>
</div>
<ol start="8" style="list-style-type: decimal">
<li>Launch the code. <code>cd CarND-Capstone-Project\ros\</code> and <code>roslaunch launch\styx.launch</code></li>
</ol>
<div class="figure">
<img src="./support/SimlatorStarted.png" title="Simulator Running" alt="alt text" />
<p class="caption">alt text</p>
</div>
<ol start="9" style="list-style-type: decimal">
<li>Clicking the <code>Camera</code> checkbox in the simulator will ready the car for autonomous mode. The green planned path with appear.</li>
</ol>
<div class="figure">
<img src="./support/FullyRunning.png" title="Fully Running" alt="alt text" />
<p class="caption">alt text</p>
</div>
<ol start="10" style="list-style-type: decimal">
<li>Now the vehicle is ready to drive autonomously arouynd the track. Click the <code>Manual</code> checkbox and the vehicle will start to drive.</li>
</ol>
<h2 id="traffic-light-detection-and-classification-end-to-end-approach-using-tensorflow">Traffic Light Detection and Classification End-to-End Approach using Tensorflow<a id="sec-6"></a></h2>
<h4 id="development-overview">Development Overview<a id="sec-6-1"></a></h4>
<p>An end-to-end approach in the traffic light detection context equates to passing the classifier an image; it then identifies the location in the scene and also categorises the traffic light state as RED, YELLOW or GREEN.</p>
<p>To achieve this I decided to develop a network model by retraining an existing model from the Tensorflow <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">model zoo</a></p>
<p>The models selected were deemed suitable for the traffic light task, based on performance and output:<br />
<strong>faster_rcnn_inception_v2_coco</strong> Speed: 60ms, Quality: 28mAP, Output: Boxes<br />
<strong>faster_rcnn_resnet101_coco</strong> Speed: 106ms, Quality: 32mAP, Output: Boxes</p>
<p>The following process was utilised to retrain the models to enable them to classify traffic lights in the simulator.</p>
<ul>
<li>Drive around simulator track and log images received from camera on rostopic /image_color. To get a range of traffic light conditions 3 Laps of track data was gathered.</li>
<li>A dataset was compiled using <a href="https://github.com/tzutalin/labelImg">labelimg</a>. Bounding boxes were drawn around the front facing traffic lights, and labelled as RED, YELLOW, GREEN or UNKOWN. Images with no traffic lights were not labelled.</li>
<li>The Object Detection libraries in Tensorflow v1.12 were required to enable re-training of the models. The dataset was converted to a tensorflow 'record' to proceed with training.</li>
<li>The basic configuration for each model in the training setup is:
<ul>
<li>Inception v1: Epoch: 2000 Input Dimensions: min:600 max:800</li>
<li>Inception v2: Epoch: 20000 Input Dimensions: min:600 max:800</li>
<li>Resnet: Epoch: 80000 Input Dimensions: min:600 max:800</li>
</ul></li>
<li>Training the models was performed using the scripts available in the Tensorflow Object library.</li>
<li>I created python-notebook pipeline to test each model against a set of images which the model had not seen during training. The notebook painted bounding boxes on each image, providing the classification and confidence. 500 images passed through produced the results for Inception v2 are shown in this <a href="https://www.youtube.com/watch?v=1QT6ahoyVDY&amp;t=124s">Video</a></li>
<li>After successful static image evaluation all models were frozen; For compatibility with Udacity environment freezing was performed using Tensorflow v1.4.</li>
<li>The frozen models were integrated into the <a href="https://github.com/nutmas/CapstoneProject-AutoWheels/blob/TensorBranch/ros/src/tl_detector/light_classification/tl_classifier.py"><code>tl_classifier.py</code></a> node of the pipeline.
<ul>
<li>From ROS camera image is received by 'tl_detector.py' and passed into a shared lockable variable.</li>
<li>The function <code>get_classification()</code> is ran in a parallel thread to process the image and utilise the classifier. This avoids the classifier impacting on the ROS processing its other tasks.</li>
<li>The classifier processes the image and returns the detection and classification results.</li>
<li>The array of classification scores for each traffic light detection are evaluated and highest confidence classification is taken as the result to pass back to <a href="https://github.com/nutmas/CapstoneProject-AutoWheels/blob/TensorBranch/ros/src/tl_detector/tl_detector.py"><code>tl_detector.py</code></a></li>
<li>In Parallel to classification thread, the <a href="https://github.com/nutmas/CapstoneProject-AutoWheels/blob/TensorBranch/ros/src/tl_detector/tl_detector.py"><code>tl_detector.py</code></a>function <code>run_main()</code> continuously calculates the nearest traffic light based on current pose, to understand the distance to next stop line. When a position and classification are aligned, the node will only output a waypoint representing distance to stop line, if the traffic light is RED or YELLOW.</li>
<li>The <a href="https://github.com/nutmas/CapstoneProject-AutoWheels/blob/TensorBranch/ros/src/waypoint_updater/waypoint_updater.py"><code>waypoint_updater.py</code></a> receives the stop line waypoint and will control the vehicle to bring it to a stop at the stop line position. Once a green light is present the waypoint is removed and the vehicle accelerates to the set speed.</li>
</ul></li>
</ul>
<h4 id="performance-evaluation">Performance Evaluation<a id="sec-6-2"></a></h4>
<ul>
<li>Inception v1 model has lower accuracy but runs faster producing results of ~330ms per classification (On 1050Ti GPU). However this required more classification outputs to establish a confirmed traffic light state.</li>
<li>Inception v2 model has very high accuracy but runs much slower ~1.5secs per classification (On 1050Ti GPU). This can work on a single state result.</li>
<li>Both models could successfully navigate the track and obey the traffic lights. However both classifications took over 1 second to have a confirmed state. v1 would sometimes mis-classify a number of times and due to the higher state change requirements could miss a red light.</li>
<li>The simulator would crash at a certain point sometimes and the styx server crash, this occurred more frequently on the v2 model. Videos showing the performance of each model are shown in the videos:
<ul>
<li><a href="https://www.youtube.com/watch?v=G_5z3RUoplA">inception v1 video</a></li>
<li><a href="https://www.youtube.com/watch?v=eRHMHTRL228&amp;t=4s">inception v2 video</a></li>
</ul></li>
<li>I evaluated the models on a 1080Ti GPU which is similar specification to the Udacity hardware. This hardware change significantly improved the speed performance time of the classifiers. The v2 dropped from 1.5s to 650ms and maintained it quality which meant ti was a good solution for successfully navigating the simulator. The results can be seen in this <a href="https://www.youtube.com/watch?v=OaNf-dULUBw">Video</a></li>
</ul>
<h4 id="conclusion-for-end-to-end-classifier">Conclusion for end-to-end classifier</h4>
<p>The v1 and v2 inception models are similar size once frozen (52MB vs 55MB). However the model which ran for 10x more epoch is significantly slower but has a much higher reliability for classification. The v2 model was chosen as it could perform to the meet the requirement of the simulator track. No real world data training or testing was performed on the classifier yet; it was therefore judged by the team that the YOLO classifier with Darknet would be more suitable for the submission. To take this end-to-end classifier forwards it would need retraining on the real world data and have a switch in the launch file to select real world or simulator world models.</p>
<h4 id="pid-tuning-parameters">PID Tuning Parameters<a id="sec-6-3"></a></h4>
<p><strong>The final values of PID controller for end-to-end Tensorflow model were the following: (KP = 0.25, KI = 0.0, KD = 0.15, MN = 0.0, MX = 0.5).</strong></p>
<h2 id="results">Results<a id="sec-7"></a></h2>
<p>This repo shows the Tensorflow classifier implementation. This has not been classified for real world traffic lights. This model will successfully navigate the simulator track using the RCNN Inception Net as the end-to-end Classifier. The integration of the classifiers with the control is aligned so that the classifiers can be exchanged with minimal code modification. The actual submission for Real World evaluation with YOLO classifier is submitted by Team Lead Diogo Silva</p>
<p><strong>This <a href="https://www.youtube.com/watch?v=HVy0eSQZLXA">Video</a> shows the end-to-end net in operation while the vehicle navigates around the simulator track.</strong></p>
<h2 id="license">License<a id="sec-8"></a></h2>
<p>For License information please see the <a href="./LICENSE">LICENSE</a> file for details</p>
<hr />
